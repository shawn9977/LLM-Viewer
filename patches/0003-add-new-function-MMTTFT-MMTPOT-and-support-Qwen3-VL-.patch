From 3ece55c8d5a310a18e25041ea53de29c08535b6f Mon Sep 17 00:00:00 2001
From: shawn9977 <shawn.zhao@intel.com>
Date: Thu, 5 Feb 2026 17:07:26 +0800
Subject: [PATCH 3/7] add new function MMTTFT MMTPOT and support
 Qwen3-VL-32B-Instruct

---
 .../Qwen/Qwen3-VL-32B-Instruct/config.json    |  62 ++++++
 backend/get_model_graph.py                    |  90 ++++++++-
 backend/model_analyzer.py                     | 188 +++++++++++++-----
 backend/model_params.py                       |   1 +
 .../src/components/left_controls/Config.vue   |   8 +-
 5 files changed, 295 insertions(+), 54 deletions(-)
 create mode 100644 backend/Qwen/Qwen3-VL-32B-Instruct/config.json

diff --git a/backend/Qwen/Qwen3-VL-32B-Instruct/config.json b/backend/Qwen/Qwen3-VL-32B-Instruct/config.json
new file mode 100644
index 0000000..29e9250
--- /dev/null
+++ b/backend/Qwen/Qwen3-VL-32B-Instruct/config.json
@@ -0,0 +1,62 @@
+{
+  "architectures": [
+    "Qwen3VLForConditionalGeneration"
+  ],
+  "image_token_id": 151655,
+  "model_type": "qwen3_vl",
+  "text_config": {
+    "attention_bias": false,
+    "attention_dropout": 0.0,
+    "bos_token_id": 151643,
+    "dtype": "bfloat16",
+    "eos_token_id": 151645,
+    "head_dim": 128,
+    "hidden_act": "silu",
+    "hidden_size": 5120,
+    "initializer_range": 0.02,
+    "intermediate_size": 25600,
+    "max_position_embeddings": 262144,
+    "model_type": "qwen3_vl_text",
+    "num_attention_heads": 64,
+    "num_hidden_layers": 64,
+    "num_key_value_heads": 8,
+    "rms_norm_eps": 1e-06,
+    "rope_scaling": {
+      "mrope_interleaved": true,
+      "mrope_section": [
+        24,
+        20,
+        20
+      ],
+      "rope_type": "default"
+    },
+    "rope_theta": 5000000,
+    "use_cache": true,
+    "vocab_size": 151936
+  },
+  "tie_word_embeddings": false,
+  "transformers_version": "4.57.0.dev0",
+  "video_token_id": 151656,
+  "vision_config": {
+    "deepstack_visual_indexes": [
+      8,
+      16,
+      24
+    ],
+    "depth": 27,
+    "hidden_act": "gelu_pytorch_tanh",
+    "hidden_size": 1152,
+    "in_channels": 3,
+    "initializer_range": 0.02,
+    "intermediate_size": 4304,
+    "model_type": "qwen3_vl",
+    "num_heads": 16,
+    "num_position_embeddings": 2304,
+    "out_hidden_size": 5120,
+    "patch_size": 16,
+    "spatial_merge_size": 2,
+    "temporal_patch_size": 2
+  },
+  "vision_end_token_id": 151653,
+  "vision_start_token_id": 151652
+}
diff --git a/backend/get_model_graph.py b/backend/get_model_graph.py
index 01d48dc..7827a1f 100644
--- a/backend/get_model_graph.py
+++ b/backend/get_model_graph.py
@@ -32,7 +32,16 @@ def get_model_graph(model_id, hardware, inference_config):
     image_size = inference_config.get("image_size")
 
     stage = inference_config["stage"]
+    if stage == "vision":
+        stage = "prefill"
     input_node_id = "vision_input" if stage == "vision" else "input"
+    graph_stage = stage
+    if stage == "multimodal_ttft":
+        graph_stage = "prefill"
+        input_node_id = "input"
+    elif stage == "multimodal_tpot":
+        graph_stage = "decode"
+        input_node_id = "input"
 
     analyzer = get_analyzer(model_id, hardware)
     result = analyzer.analyze(
@@ -61,10 +70,10 @@ def get_model_graph(model_id, hardware, inference_config):
     ]
     edges = []
 
-    def write_to_node(name, OPs, memory_access, info, input_names=[]):
+    def write_to_node(name, OPs, memory_access, info, input_names=[], node_id=None, label=None):
         node = {
-            "label": name,
-            "id": name,
+            "label": label or name,
+            "id": node_id or name,
             "description": f"OPs:{str_number(OPs)}, Access:{str_number(memory_access, 'B')}",
             "info": info,
         }
@@ -72,9 +81,77 @@ def get_model_graph(model_id, hardware, inference_config):
             node["label"] += "(GQA)"
         nodes.append(node)
         for input_name in input_names:
-            edge = {"source": input_name, "target": name}
+            edge = {"source": input_name, "target": node["id"]}
             edges.append(edge)
 
+    def add_layer_graph(layer_graph, result_stage, prefix, label_prefix, root_id=None, root_label=None, root_target=None):
+        if root_id and root_label:
+            nodes.append({"label": root_label, "id": root_id})
+        if root_id and root_target:
+            edges.append({"source": root_id, "target": f"{prefix}{root_target}"})
+        for name, input_names in layer_graph.items():
+            node_id = f"{prefix}{name}"
+            node_label = f"{label_prefix}{name}"
+            if name in ["input", "output", "vision_input", "vision_output"] or name not in result_stage:
+                OPs = 0
+                memory_access = 0
+                info = {}
+            else:
+                OPs = result_stage[name]["OPs"]
+                memory_access = result_stage[name]["memory_access"]
+                info = result_stage[name]
+            write_to_node(
+                name,
+                OPs,
+                memory_access,
+                info,
+                [f"{prefix}{n}" for n in input_names],
+                node_id=node_id,
+                label=node_label,
+            )
+
+    if stage == "multimodal_ttft":
+        total_results = result["total_results"]
+        text_stage = result["prefill"]
+        if use_flashattention:
+            text_layer_graph = analyzer.module.flashattention_transformer_layer_graph
+        else:
+            text_layer_graph = analyzer.module.transformer_layer_graph
+
+        if hasattr(analyzer.module, "vision_layer_graph"):
+            if use_flashattention and hasattr(analyzer.module, "vision_flashattention_layer_graph"):
+                vision_layer_graph = analyzer.module.vision_flashattention_layer_graph
+            else:
+                vision_layer_graph = analyzer.module.vision_layer_graph
+            vision_stage = result["vision"]
+            nodes = []
+            edges = []
+            add_layer_graph(
+                vision_layer_graph,
+                vision_stage,
+                prefix="vision::",
+                label_prefix="Vision:",
+                root_id="mm_vision_root",
+                root_label="Vision Encoder",
+                root_target="vision_input",
+            )
+            add_layer_graph(
+                text_layer_graph,
+                text_stage,
+                prefix="text::",
+                label_prefix="Text:",
+                root_id="mm_text_root",
+                root_label="Text Prefill",
+                root_target="input",
+            )
+            return nodes, edges, total_results, hardware_info
+        # If no vision graph, fall back to text prefill
+        layer_graph = text_layer_graph
+        result = text_stage
+        stage = "prefill"
+        input_node_id = "input"
+        nodes = [{"label": input_node_id, "id": input_node_id}]
+        edges = []
     if stage == "vision":
         if use_flashattention and hasattr(analyzer.module, "vision_flashattention_layer_graph"):
             layer_graph = analyzer.module.vision_flashattention_layer_graph
@@ -83,6 +160,7 @@ def get_model_graph(model_id, hardware, inference_config):
         else:
             # Fallback for non-VLM models when vision stage is selected
             stage = "prefill"
+            graph_stage = "prefill"
             input_node_id = "input"
             layer_graph = analyzer.module.transformer_layer_graph
     elif use_flashattention:
@@ -90,8 +168,8 @@ def get_model_graph(model_id, hardware, inference_config):
     else:
         layer_graph = analyzer.module.transformer_layer_graph
     total_results = result["total_results"]
-    if stage != "chat":
-        result = result[stage]
+    if graph_stage != "chat":
+        result = result[graph_stage]
     else:
         result = result["prefill"]
 
diff --git a/backend/model_analyzer.py b/backend/model_analyzer.py
index d29aeff..31e882c 100644
--- a/backend/model_analyzer.py
+++ b/backend/model_analyzer.py
@@ -813,12 +813,28 @@ class MoEAnalyzer(ModelAnalyzer):
         return self.results
 
 class VLMAnalyzer(ModelAnalyzer):
+    """视觉-语言模型分析器，用于分析 VLM 模型（如 Qwen2-VL, Qwen3-VL）的性能"""
     def __init__(self, model_id, hardware, model_params=None):
         super().__init__(model_id, hardware, model_params=model_params)
 
     def analyze(self, seqlen, batchsize, w_bit=16, a_bit=16, kv_bit=None, use_flashattention=False, kv_token_ratio=1, tp_size=1, image_size=None):
+        """
+        分析 VLM 模型的性能，包括文本分支和视觉分支
+
+        参数:
+            seqlen: 序列长度
+            batchsize: 批次大小
+            w_bit: 权重位宽
+            a_bit: 激活值位宽
+            kv_bit: KV 缓存位宽
+            use_flashattention: 是否使用 Flash Attention
+            kv_token_ratio: KV 压缩比率
+            tp_size: 张量并行大小
+            image_size: 图像尺寸（宽x高）
+        """
         assert seqlen > 0
         assert batchsize > 0
+        # 初始化结果字典，包含 decode（解码）、prefill（预填充）和 vision（视觉）三个阶段
         self.results = {"decode": {}, "prefill": {}, "vision": {}}
         if kv_bit is None:
             kv_bit = a_bit
@@ -829,35 +845,42 @@ class VLMAnalyzer(ModelAnalyzer):
         self.seqlen = seqlen
         self.tp_size = tp_size
 
+        # 计算每个数据类型的字节数
         w_byte = self.w_bit / 8
         a_byte = self.a_bit / 8
         kv_byte = self.kv_bit / 8
 
         model_params = self.model_params
 
-        # ===== Text branch (same as LLM) =====
-        num_attention_heads = self.module.get_num_attention_heads(model_params)
-        hidden_size = self.module.get_hidden_size(model_params)
-        num_key_value_heads = self.module.get_num_key_value_heads(model_params)
-        num_hidden_layers = self.module.get_num_hidden_layers(model_params)
+        # ===== 文本分支（与 LLM 相同）=====
+        # 获取模型的基本参数
+        num_attention_heads = self.module.get_num_attention_heads(model_params)  # 注意力头数量
+        hidden_size = self.module.get_hidden_size(model_params)  # 隐藏层大小
+        num_key_value_heads = self.module.get_num_key_value_heads(model_params)  # KV 头数量
+        num_hidden_layers = self.module.get_num_hidden_layers(model_params)  # 隐藏层数量
 
+        # 遍历所有线性层（如 q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj）
+        # 遍历所有线性层（如 q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj）
         for name, (ic, oc) in self.module.get_linear_layers(model_params, tp_size).items():
+            # 判断是否为 KV 投影层（k_proj 和 v_proj）
             is_kv_proj = name in ["k_proj", "v_proj"]
             is_normal_proj = not is_kv_proj
+            # 分析 decode 阶段的线性层
             self._analyze_to_results(
                 "decode",
                 name,
-                OPs=ic * oc * batchsize * 2,
-                load_weight=ic * oc * w_byte,
-                load_act=ic * batchsize * a_byte,
-                store_act=0 if is_kv_proj else oc * batchsize * a_byte,
+                OPs=ic * oc * batchsize * 2,  # 矩阵乘法的操作数（乘法+加法）
+                load_weight=ic * oc * w_byte,  # 加载权重的内存访问量
+                load_act=ic * batchsize * a_byte,  # 加载激活值的内存访问量
+                store_act=0 if is_kv_proj else oc * batchsize * a_byte,  # KV 投影层不存储激活值
                 load_kv_cache=0,
-                store_kv_cache=(0 if is_normal_proj else oc * batchsize * kv_byte),
+                store_kv_cache=(0 if is_normal_proj else oc * batchsize * kv_byte),  # KV 投影层需要存储到 KV 缓存
             )
+            # 分析 prefill 阶段的线性层
             self._analyze_to_results(
                 "prefill",
                 name,
-                OPs=ic * oc * batchsize * seqlen * 2,
+                OPs=ic * oc * batchsize * seqlen * 2,  # prefill 阶段需要处理整个序列
                 load_weight=ic * oc * w_byte,
                 load_act=ic * batchsize * seqlen * a_byte,
                 store_act=(0 if is_kv_proj else oc * batchsize * seqlen * a_byte),
@@ -865,28 +888,33 @@ class VLMAnalyzer(ModelAnalyzer):
                 store_kv_cache=(0 if is_normal_proj else oc * batchsize * seqlen * kv_byte),
             )
 
-        head_size = hidden_size // num_attention_heads
-        qk_matmul_OPs = seqlen * head_size * num_attention_heads * batchsize * 2
-        sv_matmul_OPs = 1 * head_size * seqlen * num_attention_heads * batchsize * 2
-        softmax_OPs = batchsize * num_attention_heads * seqlen * 1 * 5
+        # 计算注意力机制的参数
+        head_size = hidden_size // num_attention_heads  # 每个注意力头的维度
+        # decode 阶段的注意力计算量
+        qk_matmul_OPs = seqlen * head_size * num_attention_heads * batchsize * 2  # Q @ K^T
+        sv_matmul_OPs = 1 * head_size * seqlen * num_attention_heads * batchsize * 2  # Softmax(QK^T) @ V
+        softmax_OPs = batchsize * num_attention_heads * seqlen * 1 * 5  # Softmax 操作（5步：max, sub, exp, sum, div）
         if use_flashattention:
+            # 使用 Flash Attention 融合算子
             name = "fused_attention"
             bandwidth, max_OPS, onchip_buffer = get_hardware_info(self.hardware, self.w_bit, self.a_bit, self.kv_bit)
+            # Flash Attention-2 的分块大小计算
             block_size_r = min(math.ceil(onchip_buffer / (kv_byte * head_size)), head_size)
-            n_blocks_r = math.ceil(1 / block_size_r)
+            n_blocks_r = math.ceil(1 / block_size_r)  # 分块数量
             q_numel = 1 * head_size * batchsize * num_attention_heads * a_byte
             o_numel = 1 * seqlen * batchsize * num_attention_heads * a_byte
             self._analyze_to_results(
                 "decode",
                 name,
-                OPs=qk_matmul_OPs + sv_matmul_OPs + softmax_OPs,
+                OPs=qk_matmul_OPs + sv_matmul_OPs + softmax_OPs,  # 融合所有注意力操作
                 load_weight=0,
                 load_act=q_numel,
-                store_act=o_numel * 2,
+                store_act=o_numel * 2,  # 初始化 O 和保存 O
                 load_kv_cache=n_blocks_r * seqlen * head_size * batchsize * num_key_value_heads * kv_byte * 2,
                 store_kv_cache=0,
             )
         else:
+            # 不使用 Flash Attention，分别计算 QK、SV 和 Softmax
             self._analyze_to_results(
                 "decode",
                 "qk_matmul",
@@ -918,7 +946,9 @@ class VLMAnalyzer(ModelAnalyzer):
                 store_kv_cache=0,
             )
 
+        # 分析归一化层（如 RMSNorm 或 LayerNorm）
         for name in self.module.get_norm_layers(model_params):
+            # RMSNorm 需要 4 步操作，LayerNorm 需要 7 步操作
             norm_OPs = batchsize * hidden_size * 1 * (4 if "rmsnorm" in name else 7)
             self._analyze_to_results(
                 "decode",
@@ -931,22 +961,24 @@ class VLMAnalyzer(ModelAnalyzer):
                 store_kv_cache=0,
             )
 
+        # 分析残差连接（注意力和 MLP 的加法操作）
         for name in ["attn_add", "mlp_add"]:
             self._analyze_to_results(
                 "decode",
                 name,
-                OPs=batchsize * hidden_size,
+                OPs=batchsize * hidden_size,  # 逐元素加法
                 load_weight=0,
                 load_act=batchsize * hidden_size * a_byte,
                 store_act=batchsize * hidden_size * a_byte,
                 load_kv_cache=0,
                 store_kv_cache=0,
             )
+        # 分析 MLP 激活函数（如 SwiGLU）
         for name in ["mlp_act"]:
             self._analyze_to_results(
                 "decode",
                 name,
-                OPs=batchsize * hidden_size * 5,
+                OPs=batchsize * hidden_size * 5,  # Swish 激活函数需要 5 步操作
                 load_weight=0,
                 load_act=batchsize * hidden_size * a_byte,
                 store_act=batchsize * hidden_size * a_byte,
@@ -954,10 +986,12 @@ class VLMAnalyzer(ModelAnalyzer):
                 store_kv_cache=0,
             )
 
+        # prefill 阶段的注意力计算（处理整个序列）
         qk_matmul_OPs = seqlen * seqlen * head_size * num_attention_heads * batchsize * 2
         sv_matmul_OPs = seqlen * head_size * seqlen * num_attention_heads * batchsize * 2
         softmax_OPs = batchsize * num_attention_heads * seqlen * seqlen * 5
         if use_flashattention:
+            # prefill 阶段使用 Flash Attention
             name = "fused_attention"
             bandwidth, max_OPS, onchip_buffer = get_hardware_info(self.hardware, self.w_bit, self.a_bit, self.kv_bit)
             block_size_r = min(math.ceil(onchip_buffer / (kv_byte * head_size)), head_size)
@@ -975,6 +1009,7 @@ class VLMAnalyzer(ModelAnalyzer):
                 store_kv_cache=0,
             )
         else:
+            # prefill 阶段不使用 Flash Attention
             self._analyze_to_results(
                 "prefill",
                 "qk_matmul",
@@ -1006,6 +1041,7 @@ class VLMAnalyzer(ModelAnalyzer):
                 store_kv_cache=0,
             )
 
+        # prefill 阶段的归一化层
         for name in self.module.get_norm_layers(model_params):
             norm_OPs = batchsize * hidden_size * seqlen * (4 if "rmsnorm" in name else 7)
             self._analyze_to_results(
@@ -1018,6 +1054,7 @@ class VLMAnalyzer(ModelAnalyzer):
                 load_kv_cache=0,
                 store_kv_cache=0,
             )
+        # prefill 阶段的残差连接
         for name in ["attn_add", "mlp_add"]:
             self._analyze_to_results(
                 "prefill",
@@ -1029,6 +1066,7 @@ class VLMAnalyzer(ModelAnalyzer):
                 load_kv_cache=0,
                 store_kv_cache=0,
             )
+        # prefill 阶段的 MLP 激活函数
         for name in ["mlp_act"]:
             self._analyze_to_results(
                 "prefill",
@@ -1041,28 +1079,34 @@ class VLMAnalyzer(ModelAnalyzer):
                 store_kv_cache=0,
             )
 
+        # 计算总结果
         total_results = {"decode": {}, "prefill": {}, "vision": {}}
         for data_name in ALL_DATA_NAMES:
             total_results["decode"][data_name] = 0
             total_results["prefill"][data_name] = 0
             total_results["vision"][data_name] = 0
+        # 累加所有层的结果（乘以层数）
         for stage in ["decode", "prefill"]:
             for _, result in self.results[stage].items():
                 for data_name in ALL_DATA_NAMES:
                     total_results[stage][data_name] += result[data_name] * num_hidden_layers
 
+        # 计算内存占用
         weight_kv_footprint = total_results["prefill"]["load_weight"] + total_results["prefill"]["store_kv_cache"]
+        # decode 阶段的临时激活值内存
         decode_tmp_act = sum(result["store_act"] for result in self.results["decode"].values())
         total_results["decode"]["memory_consumption"] = decode_tmp_act + weight_kv_footprint
         total_results["decode"]["memory_consumption_tmp_act"] = decode_tmp_act
         total_results["decode"]["memory_consumption_weight"] = total_results["prefill"]["load_weight"]
         total_results["decode"]["memory_consumption_kv_cache"] = total_results["prefill"]["store_kv_cache"]
+        # prefill 阶段的临时激活值内存
         prefill_tmp_act = sum(result["store_act"] for result in self.results["prefill"].values())
         total_results["prefill"]["memory_consumption"] = prefill_tmp_act + weight_kv_footprint
         total_results["prefill"]["memory_consumption_tmp_act"] = prefill_tmp_act
         total_results["prefill"]["memory_consumption_weight"] = total_results["prefill"]["load_weight"]
         total_results["prefill"]["memory_consumption_kv_cache"] = total_results["prefill"]["store_kv_cache"]
 
+        # 处理 lm_head 层（语言模型头）
         args = {"batchsize": batchsize, "seqlen": seqlen, "a_byte": a_byte, "w_byte": w_byte}
         for layer_info in self.module.post_process(self.model_params, args):
             self._analyze_to_results(**layer_info)
@@ -1071,41 +1115,49 @@ class VLMAnalyzer(ModelAnalyzer):
                     data_name
                 ]
 
-        # ===== Vision branch =====
+        # ===== 视觉分支 =====
         def _parse_image_size(size):
+            """解析图像尺寸，支持多种输入格式"""
             if isinstance(size, dict):
+                # 字典格式：{"width": 1024, "height": 768} 或 {"w": 1024, "h": 768}
                 width = size.get("width") or size.get("w")
                 height = size.get("height") or size.get("h")
                 if width and height:
                     return int(width), int(height)
             if isinstance(size, (list, tuple)) and len(size) == 2:
+                # 列表或元组格式：[1024, 768]
                 return int(size[0]), int(size[1])
             if isinstance(size, str) and "x" in size:
+                # 字符串格式："1024x768"
                 parts = size.lower().split("x")
                 if len(parts) == 2:
                     return int(parts[0]), int(parts[1])
+            # 默认尺寸
             return 1024, 1024
 
+        # 解析图像尺寸并获取视觉编码器参数
         image_w, image_h = _parse_image_size(image_size)
-        patch_size = self.module.get_vision_patch_size(model_params)
-        spatial_merge_size = self.module.get_vision_spatial_merge_size(model_params)
-        in_channels = self.module.get_vision_in_channels(model_params)
-        vision_hidden_size = self.module.get_vision_hidden_size(model_params)
-        vision_num_heads = self.module.get_vision_num_heads(model_params)
-        vision_intermediate_size = self.module.get_vision_intermediate_size(model_params)
-        vision_num_layers = self.module.get_vision_num_hidden_layers(model_params)
-
-        num_patches_w = max(1, math.ceil(image_w / patch_size))
-        num_patches_h = max(1, math.ceil(image_h / patch_size))
-        num_patches = num_patches_w * num_patches_h
-        merged_tokens = max(1, math.ceil(num_patches / max(1, spatial_merge_size) ** 2))
-
-        patch_ic = in_channels * patch_size * patch_size
-        patch_oc = vision_hidden_size
+        patch_size = self.module.get_vision_patch_size(model_params)  # 图像块大小（如 14x14）
+        spatial_merge_size = self.module.get_vision_spatial_merge_size(model_params)  # 空间合并大小
+        in_channels = self.module.get_vision_in_channels(model_params)  # 输入通道数（通常为 3，RGB）
+        vision_hidden_size = self.module.get_vision_hidden_size(model_params)  # 视觉编码器隐藏层大小
+        vision_num_heads = self.module.get_vision_num_heads(model_params)  # 视觉编码器注意力头数量
+        vision_intermediate_size = self.module.get_vision_intermediate_size(model_params)  # 视觉编码器中间层大小
+        vision_num_layers = self.module.get_vision_num_hidden_layers(model_params)  # 视觉编码器层数
+
+        # 计算图像块数量和合并后的 token 数量
+        num_patches_w = max(1, math.ceil(image_w / patch_size))  # 宽度方向的图像块数量
+        num_patches_h = max(1, math.ceil(image_h / patch_size))  # 高度方向的图像块数量
+        num_patches = num_patches_w * num_patches_h  # 总图像块数量
+        merged_tokens = max(1, math.ceil(num_patches / max(1, spatial_merge_size) ** 2))  # 空间合并后的 token 数量
+
+        # 分析图像块嵌入层（Patch Embedding）
+        patch_ic = in_channels * patch_size * patch_size  # 输入通道数（如 3*14*14=588）
+        patch_oc = vision_hidden_size  # 输出通道数
         self._analyze_to_results(
             "vision",
             "vision_patch_embed",
-            OPs=patch_ic * patch_oc * batchsize * num_patches * 2,
+            OPs=patch_ic * patch_oc * batchsize * num_patches * 2,  # 卷积或线性投影的计算量
             load_weight=patch_ic * patch_oc * w_byte,
             load_act=patch_ic * batchsize * num_patches * a_byte,
             store_act=patch_oc * batchsize * num_patches * a_byte,
@@ -1113,11 +1165,12 @@ class VLMAnalyzer(ModelAnalyzer):
             store_kv_cache=0,
         )
 
+        # 分析视觉编码器的线性层（Q、K、V 投影和 MLP 层）
         for name, (ic, oc) in self.module.get_vision_linear_layers(model_params, tp_size).items():
             self._analyze_to_results(
                 "vision",
                 name,
-                OPs=ic * oc * batchsize * merged_tokens * 2,
+                OPs=ic * oc * batchsize * merged_tokens * 2,  # 线性层的计算量
                 load_weight=ic * oc * w_byte,
                 load_act=ic * batchsize * merged_tokens * a_byte,
                 store_act=oc * batchsize * merged_tokens * a_byte,
@@ -1125,18 +1178,20 @@ class VLMAnalyzer(ModelAnalyzer):
                 store_kv_cache=0,
             )
 
-        vision_head_size = vision_hidden_size // vision_num_heads
-        v_qk_OPs = merged_tokens * merged_tokens * vision_head_size * vision_num_heads * batchsize * 2
-        v_sv_OPs = merged_tokens * vision_head_size * merged_tokens * vision_num_heads * batchsize * 2
-        v_softmax_OPs = batchsize * vision_num_heads * merged_tokens * merged_tokens * 5
+        # 分析视觉编码器的注意力机制
+        vision_head_size = vision_hidden_size // vision_num_heads  # 每个注意力头的维度
+        v_qk_OPs = merged_tokens * merged_tokens * vision_head_size * vision_num_heads * batchsize * 2  # Q @ K^T
+        v_sv_OPs = merged_tokens * vision_head_size * merged_tokens * vision_num_heads * batchsize * 2  # Softmax(QK^T) @ V
+        v_softmax_OPs = batchsize * vision_num_heads * merged_tokens * merged_tokens * 5  # Softmax 操作
 
         if use_flashattention:
+            # 视觉编码器使用 Flash Attention
             name = "vision_fused_attention"
             bandwidth, max_OPS, onchip_buffer = get_hardware_info(self.hardware, self.w_bit, self.a_bit, self.kv_bit)
             block_size_r = min(math.ceil(onchip_buffer / (a_byte * vision_head_size)), vision_head_size)
             n_blocks_r = math.ceil(merged_tokens / block_size_r)
             q_numel = merged_tokens * vision_head_size * batchsize * vision_num_heads * a_byte
-            kv_numel = merged_tokens * vision_head_size * batchsize * vision_num_heads * a_byte * 2
+            kv_numel = merged_tokens * vision_head_size * batchsize * vision_num_heads * a_byte * 2  # K 和 V
             o_numel = merged_tokens * merged_tokens * batchsize * vision_num_heads * a_byte
             self._analyze_to_results(
                 "vision",
@@ -1149,6 +1204,7 @@ class VLMAnalyzer(ModelAnalyzer):
                 store_kv_cache=0,
             )
         else:
+            # 视觉编码器不使用 Flash Attention
             self._analyze_to_results(
                 "vision",
                 "vision_qk_matmul",
@@ -1180,8 +1236,9 @@ class VLMAnalyzer(ModelAnalyzer):
                 store_kv_cache=0,
             )
 
+        # 分析视觉编码器的归一化层
         for name in self.module.get_vision_norm_layers(model_params):
-            norm_OPs = batchsize * vision_hidden_size * merged_tokens * 7
+            norm_OPs = batchsize * vision_hidden_size * merged_tokens * 7  # LayerNorm 需要 7 步操作
             self._analyze_to_results(
                 "vision",
                 name,
@@ -1193,6 +1250,7 @@ class VLMAnalyzer(ModelAnalyzer):
                 store_kv_cache=0,
             )
 
+        # 分析视觉编码器的残差连接
         for name in ["vision_attn_add", "vision_mlp_add"]:
             self._analyze_to_results(
                 "vision",
@@ -1205,6 +1263,7 @@ class VLMAnalyzer(ModelAnalyzer):
                 store_kv_cache=0,
             )
 
+        # 分析视觉编码器的 MLP 激活函数
         self._analyze_to_results(
             "vision",
             "vision_mlp_act",
@@ -1216,6 +1275,7 @@ class VLMAnalyzer(ModelAnalyzer):
             store_kv_cache=0,
         )
 
+        # 定义需要重复计算的视觉层（这些层在每个 Transformer 块中都会出现）
         vision_repeat_layers = {
             "vision_q_proj",
             "vision_k_proj",
@@ -1235,11 +1295,13 @@ class VLMAnalyzer(ModelAnalyzer):
             "vision_fused_attention",
         }
 
+        # 累加视觉分支的结果（重复层需要乘以层数）
         for name, result in self.results["vision"].items():
             multiplier = vision_num_layers if name in vision_repeat_layers else 1
             for data_name in ALL_DATA_NAMES:
                 total_results["vision"][data_name] += result[data_name] * multiplier
 
+        # 处理视觉编码器的后处理层（如投影层）
         vision_args = {"batchsize": batchsize, "seqlen": merged_tokens, "a_byte": a_byte, "w_byte": w_byte}
         for layer_info in self.module.vision_post_process(self.model_params, vision_args):
             self._analyze_to_results(**layer_info)
@@ -1248,6 +1310,7 @@ class VLMAnalyzer(ModelAnalyzer):
                     data_name
                 ]
 
+        # 计算视觉分支的内存占用
         vision_tmp_act = 0
         for name, result in self.results["vision"].items():
             multiplier = vision_num_layers if name in vision_repeat_layers else 1
@@ -1256,8 +1319,43 @@ class VLMAnalyzer(ModelAnalyzer):
         total_results["vision"]["memory_consumption"] = vision_tmp_act + vision_weight
         total_results["vision"]["memory_consumption_tmp_act"] = vision_tmp_act
         total_results["vision"]["memory_consumption_weight"] = vision_weight
-        total_results["vision"]["memory_consumption_kv_cache"] = 0
+        total_results["vision"]["memory_consumption_kv_cache"] = 0  # 视觉编码器不使用 KV 缓存
+
+        # ===== 多模态总计 =====
+        # 计算 TTFT（Time To First Token，首 token 时间）和 TPOT（Time Per Output Token，每 token 时间）
+        total_results["multimodal_ttft"] = {}  # TTFT = 视觉编码 + 文本预填充
+        total_results["multimodal_tpot"] = {}  # TPOT = 文本解码
+        for data_name in ALL_DATA_NAMES:
+            # TTFT 包含视觉分支和文本 prefill 分支的计算量
+            total_results["multimodal_ttft"][data_name] = (
+                total_results["vision"][data_name] + total_results["prefill"][data_name]
+            )
+            # TPOT 只包含文本 decode 分支的计算量
+            total_results["multimodal_tpot"][data_name] = total_results["decode"][data_name]
 
+        # TTFT 阶段的内存占用：权重相加，临时激活值取最大值，KV 缓存来自文本 prefill
+        ttft_weight = (
+            total_results["vision"]["memory_consumption_weight"] +
+            total_results["prefill"]["memory_consumption_weight"]
+        )
+        # 临时激活值取最大值（因为视觉和文本不会同时占用内存）
+        ttft_tmp_act = max(
+            total_results["vision"]["memory_consumption_tmp_act"],
+            total_results["prefill"]["memory_consumption_tmp_act"],
+        )
+        ttft_kv = total_results["prefill"]["memory_consumption_kv_cache"]
+        total_results["multimodal_ttft"]["memory_consumption_weight"] = ttft_weight
+        total_results["multimodal_ttft"]["memory_consumption_tmp_act"] = ttft_tmp_act
+        total_results["multimodal_ttft"]["memory_consumption_kv_cache"] = ttft_kv
+        total_results["multimodal_ttft"]["memory_consumption"] = ttft_weight + ttft_tmp_act + ttft_kv
+
+        # TPOT 阶段的内存占用（与文本 decode 相同）
+        total_results["multimodal_tpot"]["memory_consumption"] = total_results["decode"]["memory_consumption"]
+        total_results["multimodal_tpot"]["memory_consumption_weight"] = total_results["decode"]["memory_consumption_weight"]
+        total_results["multimodal_tpot"]["memory_consumption_tmp_act"] = total_results["decode"]["memory_consumption_tmp_act"]
+        total_results["multimodal_tpot"]["memory_consumption_kv_cache"] = total_results["decode"]["memory_consumption_kv_cache"]
+
+        # 保存总结果并返回
         self.results["total_results"] = total_results
         return self.results
 
diff --git a/backend/model_params.py b/backend/model_params.py
index bd3b347..6f9ee75 100644
--- a/backend/model_params.py
+++ b/backend/model_params.py
@@ -10,6 +10,7 @@ available_model_ids_sources = {
     "Qwen/Qwen3-4B-Instruct-2507": {"source": "huggingface"},
     "Qwen/Qwen3-32B": {"source": "huggingface"},
     "Qwen/Qwen3-30B-A3B-Instruct-2507": {"source": "huggingface"},
+    "Qwen/Qwen3-VL-32B-Instruct": {"source": "huggingface"},
     "Qwen/Qwen3-VL-8B-Instruct": {"source": "huggingface"},
     "LLM-Research/llama-2-7b": {"source": "modelscope"},
     "LLM-Research/llama-2-13b": {"source": "modelscope"},
diff --git a/frontend/src/components/left_controls/Config.vue b/frontend/src/components/left_controls/Config.vue
index fce23ba..46d857d 100644
--- a/frontend/src/components/left_controls/Config.vue
+++ b/frontend/src/components/left_controls/Config.vue
@@ -8,15 +8,17 @@
         <label for="prefill">Prefill</label>
         <input type="radio" v-model="inference_stage" id="chat" value="chat">
         <label for="prefill">Chat</label>
-        <input type="radio" v-model="inference_stage" id="vision" value="vision">
-        <label for="vision">Vision</label>
+        <input type="radio" v-model="inference_stage" id="multimodal_ttft" value="multimodal_ttft">
+        <label for="multimodal_ttft">MM TTFT</label>
+        <input type="radio" v-model="inference_stage" id="multimodal_tpot" value="multimodal_tpot">
+        <label for="multimodal_tpot">MM TPOT</label>
     </div>
     <div class="config_div">
         Batchsize:
         <input type="range" min="1" max="256" value="1" v-model.lazy="batch_size">
         <input type="number" v-model.lazy="batch_size" min="1" max="256">
     </div>
-    <div class="config_div" v-if="inference_stage=='vision'">
+    <div class="config_div" v-if="inference_stage=='multimodal_ttft'">
         SeqLength:
         <input type="range" min="1" max="4096" value="1024" v-model.lazy="seq_length">
         <input type="number" v-model.lazy="seq_length" min="1" max="4096">
-- 
2.34.1

