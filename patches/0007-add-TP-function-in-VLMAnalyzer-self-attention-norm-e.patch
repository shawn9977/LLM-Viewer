From 2b2e996d312ce51fd78382aa2cac45c0a3e34e2c Mon Sep 17 00:00:00 2001
From: shawn9977 <shawn.zhao@intel.com>
Date: Tue, 10 Feb 2026 16:23:39 +0800
Subject: [PATCH 7/7] add TP function in VLMAnalyzer self-attention,norm etc
 element operator

---
 backend/model_analyzer.py | 107 +++++++++++++++++++++-----------------
 1 file changed, 59 insertions(+), 48 deletions(-)

diff --git a/backend/model_analyzer.py b/backend/model_analyzer.py
index 6fb9add..f61d0d8 100644
--- a/backend/model_analyzer.py
+++ b/backend/model_analyzer.py
@@ -863,7 +863,6 @@ class VLMAnalyzer(ModelAnalyzer):
         num_key_value_heads = self.module.get_num_key_value_heads(model_params)  # KV 头数量
         num_hidden_layers = self.module.get_num_hidden_layers(model_params)  # 隐藏层数量
 
-        # 遍历所有线性层（如 q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj）
         # 遍历所有线性层（如 q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj）
         for name, (ic, oc) in self.module.get_linear_layers(model_params, tp_size).items():
             # 判断是否为 KV 投影层（k_proj 和 v_proj）
@@ -894,19 +893,26 @@ class VLMAnalyzer(ModelAnalyzer):
 
         # 计算注意力机制的参数
         head_size = hidden_size // num_attention_heads  # 每个注意力头的维度
+        tp_num_attention_heads = max(1, num_attention_heads // tp_size)  # TP 后的 Q heads
+        tp_num_key_value_heads = max(1, num_key_value_heads // tp_size)  # TP 后的 KV heads
         # decode 阶段的注意力计算量
-        qk_matmul_OPs = seqlen * head_size * num_attention_heads * batchsize * 2  # Q @ K^T
-        sv_matmul_OPs = 1 * head_size * seqlen * num_attention_heads * batchsize * 2  # Softmax(QK^T) @ V
-        softmax_OPs = batchsize * num_attention_heads * seqlen * 1 * 5  # Softmax 操作（5步：max, sub, exp, sum, div）
+        qk_matmul_OPs = seqlen * head_size * tp_num_attention_heads * batchsize * 2  # Q @ K^T
+        sv_matmul_OPs = 1 * head_size * seqlen * tp_num_attention_heads * batchsize * 2  # Softmax(QK^T) @ V
+        softmax_OPs = batchsize * tp_num_attention_heads * seqlen * 1 * 5  # Softmax 操作（5步：max, sub, exp, sum, div）
         if use_flashattention:
             # 使用 Flash Attention 融合算子
             name = "fused_attention"
             bandwidth, max_OPS, onchip_buffer = get_hardware_info(self.hardware, self.w_bit, self.a_bit, self.kv_bit)
             # Flash Attention-2 的分块大小计算
             block_size_r = min(math.ceil(onchip_buffer / (kv_byte * head_size)), head_size)
-            n_blocks_r = math.ceil(1 / block_size_r)  # 分块数量
-            q_numel = 1 * head_size * batchsize * num_attention_heads * a_byte
-            o_numel = 1 * seqlen * batchsize * num_attention_heads * a_byte
+            n_blocks_r = math.ceil(seqlen / block_size_r)  # 分块数量
+            q_numel = 1 * head_size * batchsize * tp_num_attention_heads * a_byte
+            o_numel = 1 * seqlen * batchsize * tp_num_attention_heads * a_byte
+
+            # GQA optimization: calculate KV reuse factor
+            num_queries_per_kv = tp_num_attention_heads // tp_num_key_value_heads if tp_num_key_value_heads > 0 else 1
+            kv_reuse_factor = min(num_queries_per_kv, max(1, block_size_r // head_size)) if num_queries_per_kv > 1 else 1
+
             self._analyze_to_results(
                 "decode",
                 name,
@@ -914,7 +920,7 @@ class VLMAnalyzer(ModelAnalyzer):
                 load_weight=0,
                 load_act=q_numel,
                 store_act=o_numel * 2,  # 初始化 O 和保存 O
-                load_kv_cache=n_blocks_r * seqlen * head_size * batchsize * num_key_value_heads * kv_byte * 2,
+                load_kv_cache=n_blocks_r * seqlen * head_size * batchsize * tp_num_key_value_heads * kv_byte * 2 / kv_reuse_factor,
                 store_kv_cache=0,
             )
         else:
@@ -924,9 +930,9 @@ class VLMAnalyzer(ModelAnalyzer):
                 "qk_matmul",
                 OPs=qk_matmul_OPs,
                 load_weight=0,
-                load_act=1 * head_size * batchsize * num_attention_heads * a_byte,
-                store_act=1 * seqlen * batchsize * num_attention_heads * a_byte,
-                load_kv_cache=seqlen * head_size * batchsize * num_key_value_heads * kv_byte,
+                load_act=1 * head_size * batchsize * tp_num_attention_heads * a_byte,
+                store_act=1 * seqlen * batchsize * tp_num_attention_heads * a_byte,
+                load_kv_cache=seqlen * head_size * batchsize * tp_num_key_value_heads * kv_byte,
                 store_kv_cache=0,
             )
             self._analyze_to_results(
@@ -934,9 +940,9 @@ class VLMAnalyzer(ModelAnalyzer):
                 "sv_matmul",
                 OPs=sv_matmul_OPs,
                 load_weight=0,
-                load_act=1 * seqlen * batchsize * num_attention_heads * a_byte,
-                store_act=1 * head_size * batchsize * num_attention_heads * a_byte,
-                load_kv_cache=seqlen * head_size * batchsize * num_key_value_heads * kv_byte,
+                load_act=1 * seqlen * batchsize * tp_num_attention_heads * a_byte,
+                store_act=1 * head_size * batchsize * tp_num_attention_heads * a_byte,
+                load_kv_cache=seqlen * head_size * batchsize * tp_num_key_value_heads * kv_byte,
                 store_kv_cache=0,
             )
             self._analyze_to_results(
@@ -944,8 +950,8 @@ class VLMAnalyzer(ModelAnalyzer):
                 "softmax",
                 OPs=softmax_OPs,
                 load_weight=0,
-                load_act=batchsize * num_attention_heads * seqlen * a_byte,
-                store_act=batchsize * num_attention_heads * seqlen * a_byte,
+                load_act=batchsize * tp_num_attention_heads * seqlen * a_byte,
+                store_act=batchsize * tp_num_attention_heads * seqlen * a_byte,
                 load_kv_cache=0,
                 store_kv_cache=0,
             )
@@ -953,14 +959,14 @@ class VLMAnalyzer(ModelAnalyzer):
         # 分析归一化层（如 RMSNorm 或 LayerNorm）
         for name in self.module.get_norm_layers(model_params):
             # RMSNorm 需要 4 步操作，LayerNorm 需要 7 步操作
-            norm_OPs = batchsize * hidden_size * 1 * (4 if "rmsnorm" in name else 7)
+            norm_OPs = batchsize * (hidden_size / tp_size) * 1 * (4 if "rmsnorm" in name else 7)
             self._analyze_to_results(
                 "decode",
                 name,
                 OPs=norm_OPs,
                 load_weight=0,
-                load_act=batchsize * hidden_size * a_byte,
-                store_act=batchsize * hidden_size * a_byte,
+                load_act=batchsize * (hidden_size / tp_size) * a_byte,
+                store_act=batchsize * (hidden_size / tp_size) * a_byte,
                 load_kv_cache=0,
                 store_kv_cache=0,
             )
@@ -970,10 +976,10 @@ class VLMAnalyzer(ModelAnalyzer):
             self._analyze_to_results(
                 "decode",
                 name,
-                OPs=batchsize * hidden_size,  # 逐元素加法
+                OPs=batchsize * (hidden_size / tp_size),  # 逐元素加法
                 load_weight=0,
-                load_act=batchsize * hidden_size * a_byte,
-                store_act=batchsize * hidden_size * a_byte,
+                load_act=batchsize * (hidden_size / tp_size) * a_byte,
+                store_act=batchsize * (hidden_size / tp_size) * a_byte,
                 load_kv_cache=0,
                 store_kv_cache=0,
             )
@@ -982,26 +988,31 @@ class VLMAnalyzer(ModelAnalyzer):
             self._analyze_to_results(
                 "decode",
                 name,
-                OPs=batchsize * hidden_size * 5,  # Swish 激活函数需要 5 步操作
+                OPs=batchsize * (hidden_size / tp_size) * 5,  # Swish 激活函数需要 5 步操作
                 load_weight=0,
-                load_act=batchsize * hidden_size * a_byte,
-                store_act=batchsize * hidden_size * a_byte,
+                load_act=batchsize * (hidden_size / tp_size) * a_byte,
+                store_act=batchsize * (hidden_size / tp_size) * a_byte,
                 load_kv_cache=0,
                 store_kv_cache=0,
             )
 
         # prefill 阶段的注意力计算（处理整个序列）
-        qk_matmul_OPs = seqlen * seqlen * head_size * num_attention_heads * batchsize * 2
-        sv_matmul_OPs = seqlen * head_size * seqlen * num_attention_heads * batchsize * 2
-        softmax_OPs = batchsize * num_attention_heads * seqlen * seqlen * 5
+        qk_matmul_OPs = seqlen * seqlen * head_size * tp_num_attention_heads * batchsize * 2
+        sv_matmul_OPs = seqlen * head_size * seqlen * tp_num_attention_heads * batchsize * 2
+        softmax_OPs = batchsize * tp_num_attention_heads * seqlen * seqlen * 5
         if use_flashattention:
             # prefill 阶段使用 Flash Attention
             name = "fused_attention"
             bandwidth, max_OPS, onchip_buffer = get_hardware_info(self.hardware, self.w_bit, self.a_bit, self.kv_bit)
             block_size_r = min(math.ceil(onchip_buffer / (kv_byte * head_size)), head_size)
             n_blocks_r = math.ceil(seqlen / block_size_r)
-            q_numel = seqlen * head_size * batchsize * num_attention_heads * a_byte
-            o_numel = seqlen * seqlen * batchsize * num_attention_heads * a_byte
+            q_numel = seqlen * head_size * batchsize * tp_num_attention_heads * a_byte
+            o_numel = seqlen * seqlen * batchsize * tp_num_attention_heads * a_byte
+
+            # GQA optimization: calculate KV reuse factor
+            num_queries_per_kv = tp_num_attention_heads // tp_num_key_value_heads if tp_num_key_value_heads > 0 else 1
+            kv_reuse_factor = min(num_queries_per_kv, max(1, block_size_r // head_size)) if num_queries_per_kv > 1 else 1
+
             self._analyze_to_results(
                 "prefill",
                 name,
@@ -1009,7 +1020,7 @@ class VLMAnalyzer(ModelAnalyzer):
                 load_weight=0,
                 load_act=q_numel,
                 store_act=o_numel * 2,
-                load_kv_cache=n_blocks_r * seqlen * head_size * batchsize * num_key_value_heads * kv_byte * 2,
+                load_kv_cache=n_blocks_r * seqlen * head_size * batchsize * tp_num_key_value_heads * kv_byte * 2 / kv_reuse_factor,
                 store_kv_cache=0,
             )
         else:
@@ -1019,9 +1030,9 @@ class VLMAnalyzer(ModelAnalyzer):
                 "qk_matmul",
                 OPs=qk_matmul_OPs,
                 load_weight=0,
-                load_act=seqlen * head_size * batchsize * num_key_value_heads * a_byte,
-                store_act=seqlen * seqlen * batchsize * num_attention_heads * a_byte,
-                load_kv_cache=seqlen * head_size * batchsize * num_key_value_heads * kv_byte,
+                load_act=seqlen * head_size * batchsize * tp_num_key_value_heads * a_byte,
+                store_act=seqlen * seqlen * batchsize * tp_num_attention_heads * a_byte,
+                load_kv_cache=seqlen * head_size * batchsize * tp_num_key_value_heads * kv_byte,
                 store_kv_cache=0,
             )
             self._analyze_to_results(
@@ -1029,9 +1040,9 @@ class VLMAnalyzer(ModelAnalyzer):
                 "sv_matmul",
                 OPs=sv_matmul_OPs,
                 load_weight=0,
-                load_act=seqlen * seqlen * batchsize * num_attention_heads * a_byte,
-                store_act=seqlen * head_size * batchsize * num_attention_heads * a_byte,
-                load_kv_cache=seqlen * head_size * batchsize * num_key_value_heads * kv_byte,
+                load_act=seqlen * seqlen * batchsize * tp_num_attention_heads * a_byte,
+                store_act=seqlen * head_size * batchsize * tp_num_attention_heads * a_byte,
+                load_kv_cache=seqlen * head_size * batchsize * tp_num_key_value_heads * kv_byte,
                 store_kv_cache=0,
             )
             self._analyze_to_results(
@@ -1039,22 +1050,22 @@ class VLMAnalyzer(ModelAnalyzer):
                 "softmax",
                 OPs=softmax_OPs,
                 load_weight=0,
-                load_act=batchsize * num_attention_heads * seqlen * seqlen * a_byte,
-                store_act=batchsize * num_attention_heads * seqlen * seqlen * a_byte,
+                load_act=batchsize * tp_num_attention_heads * seqlen * seqlen * a_byte,
+                store_act=batchsize * tp_num_attention_heads * seqlen * seqlen * a_byte,
                 load_kv_cache=0,
                 store_kv_cache=0,
             )
 
         # prefill 阶段的归一化层
         for name in self.module.get_norm_layers(model_params):
-            norm_OPs = batchsize * hidden_size * seqlen * (4 if "rmsnorm" in name else 7)
+            norm_OPs = batchsize * (hidden_size / tp_size) * seqlen * (4 if "rmsnorm" in name else 7)
             self._analyze_to_results(
                 "prefill",
                 name,
                 OPs=norm_OPs,
                 load_weight=0,
-                load_act=batchsize * hidden_size * seqlen * a_byte,
-                store_act=batchsize * hidden_size * seqlen * a_byte,
+                load_act=batchsize * (hidden_size / tp_size) * seqlen * a_byte,
+                store_act=batchsize * (hidden_size / tp_size) * seqlen * a_byte,
                 load_kv_cache=0,
                 store_kv_cache=0,
             )
@@ -1063,10 +1074,10 @@ class VLMAnalyzer(ModelAnalyzer):
             self._analyze_to_results(
                 "prefill",
                 name,
-                OPs=batchsize * hidden_size * seqlen,
+                OPs=batchsize * (hidden_size / tp_size) * seqlen,
                 load_weight=0,
-                load_act=batchsize * hidden_size * seqlen * a_byte,
-                store_act=batchsize * hidden_size * seqlen * a_byte,
+                load_act=batchsize * (hidden_size / tp_size) * seqlen * a_byte,
+                store_act=batchsize * (hidden_size / tp_size) * seqlen * a_byte,
                 load_kv_cache=0,
                 store_kv_cache=0,
             )
@@ -1075,10 +1086,10 @@ class VLMAnalyzer(ModelAnalyzer):
             self._analyze_to_results(
                 "prefill",
                 name,
-                OPs=batchsize * hidden_size * seqlen * 5,
+                OPs=batchsize * (hidden_size / tp_size) * seqlen * 5,
                 load_weight=0,
-                load_act=batchsize * hidden_size * seqlen * a_byte,
-                store_act=batchsize * hidden_size * seqlen * a_byte,
+                load_act=batchsize * (hidden_size / tp_size) * seqlen * a_byte,
+                store_act=batchsize * (hidden_size / tp_size) * seqlen * a_byte,
                 load_kv_cache=0,
                 store_kv_cache=0,
             )
-- 
2.34.1

