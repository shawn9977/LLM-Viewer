From f7ab8ac6079985fce4884ce5e7a5c20bb1e2c10d Mon Sep 17 00:00:00 2001
From: shawn9977 <shawn.zhao@intel.com>
Date: Mon, 9 Feb 2026 16:36:44 +0800
Subject: [PATCH 5/7] add TP in QK/SV/Softmax to reduce prefill

---
 .../Qwen3-Omni-30B-A3B-Instruct/config.json   | 485 ++++++++++++++++++
 backend/model_analyzer.py                     | 116 +++--
 2 files changed, 545 insertions(+), 56 deletions(-)
 create mode 100644 backend/Qwen/Qwen3-Omni-30B-A3B-Instruct/config.json

diff --git a/backend/Qwen/Qwen3-Omni-30B-A3B-Instruct/config.json b/backend/Qwen/Qwen3-Omni-30B-A3B-Instruct/config.json
new file mode 100644
index 0000000..18c714a
--- /dev/null
+++ b/backend/Qwen/Qwen3-Omni-30B-A3B-Instruct/config.json
@@ -0,0 +1,485 @@
+{
+  "architectures": [
+    "Qwen3OmniMoeForConditionalGeneration"
+  ],
+  "assistant_token_id": 77091,
+  "code2wav_config": {
+    "attention_bias": false,
+    "attention_dropout": 0.0,
+    "codebook_dim": 512,
+    "codebook_size": 2048,
+    "decoder_dim": 1536,
+    "hidden_act": "silu",
+    "hidden_size": 1024,
+    "intermediate_size": 3072,
+    "layer_scale_initial_scale": 0.01,
+    "max_position_embeddings": 8000,
+    "model_type": "",
+    "num_attention_heads": 16,
+    "num_hidden_layers": 8,
+    "num_key_value_heads": 16,
+    "num_quantizers": 16,
+    "num_semantic_quantizers": 1,
+    "rms_norm_eps": 1e-05,
+    "rope_theta": 10000,
+    "semantic_codebook_size": 4096,
+    "sliding_window": 72,
+    "upsample_rates": [
+      8,
+      5,
+      4,
+      3
+    ],
+    "upsampling_ratios": [
+      2,
+      2
+    ],
+    "vector_quantization_hidden_dimension": 512
+  },
+  "dtype": "bfloat16",
+  "enable_audio_output": true,
+  "im_end_token_id": 151645,
+  "im_start_token_id": 151644,
+  "model_type": "qwen3_omni_moe",
+  "system_token_id": 8948,
+  "talker_config": {
+    "text_config":{
+      "attention_bias": false,
+      "attention_dropout": 0,
+      "decoder_sparse_step": 1,
+      "head_dim": 128,
+      "hidden_act": "silu",
+      "hidden_size": 1024,
+      "initializer_range": 0.02,
+      "intermediate_size": 2048,
+      "max_position_embeddings": 65536,
+      "mlp_only_layers": [],
+      "moe_intermediate_size": 384,
+      "norm_topk_prob": true,
+      "num_attention_heads": 16,
+      "num_experts": 128,
+      "num_experts_per_tok": 6,
+      "num_hidden_layers": 20,
+      "num_key_value_heads": 2,
+      "rms_norm_eps": 1e-06,
+      "rope_scaling": {
+        "interleaved": true,
+        "mrope_section": [
+          24,
+          20,
+          20
+        ],
+        "rope_type": "default",
+        "type": "default"
+      },
+      "rope_theta": 1000000,
+      "router_aux_loss_coef": 0.001,
+      "shared_expert_intermediate_size": 768,
+      "sliding_window": null,
+      "use_cache": true,
+      "use_sliding_window": false,
+      "vocab_size": 3072
+    },
+    "accept_hidden_layer": 24,
+    "audio_end_token_id": 151670,
+    "audio_start_token_id": 151669,
+    "audio_token_id": 151675,
+    "code_predictor_config": {
+      "_name_or_path": "",
+      "add_cross_attention": false,
+      "architectures": null,
+      "attention_bias": false,
+      "attention_dropout": 0,
+      "bad_words_ids": null,
+      "begin_suppress_tokens": null,
+      "bos_token_id": null,
+      "chunk_size_feed_forward": 0,
+      "cross_attention_hidden_size": null,
+      "decoder_start_token_id": null,
+      "diversity_penalty": 0.0,
+      "do_sample": false,
+      "dtype": null,
+      "early_stopping": false,
+      "encoder_no_repeat_ngram_size": 0,
+      "eos_token_id": null,
+      "exponential_decay_length_penalty": null,
+      "finetuning_task": null,
+      "forced_bos_token_id": null,
+      "forced_eos_token_id": null,
+      "head_dim": 128,
+      "hidden_act": "silu",
+      "hidden_size": 1024,
+      "id2label": {
+        "0": "LABEL_0",
+        "1": "LABEL_1"
+      },
+      "initializer_range": 0.02,
+      "intermediate_size": 3072,
+      "is_decoder": false,
+      "is_encoder_decoder": false,
+      "label2id": {
+        "LABEL_0": 0,
+        "LABEL_1": 1
+      },
+      "layer_types": [
+        "full_attention",
+        "full_attention",
+        "full_attention",
+        "full_attention",
+        "full_attention"
+      ],
+      "length_penalty": 1.0,
+      "max_length": 20,
+      "max_position_embeddings": 32768,
+      "max_window_layers": 28,
+      "min_length": 0,
+      "model_type": "qwen3_omni_moe_talker_code_predictor",
+      "no_repeat_ngram_size": 0,
+      "num_attention_heads": 16,
+      "num_beam_groups": 1,
+      "num_beams": 1,
+      "num_code_groups": 16,
+      "num_hidden_layers": 5,
+      "num_key_value_heads": 8,
+      "num_return_sequences": 1,
+      "output_attentions": false,
+      "output_hidden_states": false,
+      "output_scores": false,
+      "pad_token_id": null,
+      "prefix": null,
+      "problem_type": null,
+      "pruned_heads": {},
+      "remove_invalid_values": false,
+      "repetition_penalty": 1.0,
+      "return_dict": true,
+      "return_dict_in_generate": false,
+      "rms_norm_eps": 1e-06,
+      "rope_scaling": null,
+      "rope_theta": 1000000,
+      "sep_token_id": null,
+      "sliding_window": null,
+      "suppress_tokens": null,
+      "task_specific_params": null,
+      "temperature": 1.0,
+      "tf_legacy_loss": false,
+      "tie_encoder_decoder": false,
+      "tie_word_embeddings": false,
+      "tokenizer_class": null,
+      "top_k": 50,
+      "top_p": 1.0,
+      "torchscript": false,
+      "typical_p": 1.0,
+      "use_bfloat16": false,
+      "use_cache": true,
+      "use_sliding_window": false,
+      "vocab_size": 2048
+    },
+    "codec_bos_id": 2149,
+    "codec_eos_token_id": 2150,
+    "codec_nothink_id": 2155,
+    "codec_pad_id": 2148,
+    "codec_think_bos_id": 2156,
+    "codec_think_eos_id": 2157,
+    "image_token_id": 151655,
+    "model_type": "qwen3_omni_moe_talker",
+    "num_code_groups": 16,
+    "output_router_logits": false,
+    "position_id_per_seconds": 13,
+    "seconds_per_chunk": 2,
+    "spatial_merge_size": 2,
+    "speaker_id": {
+      "chelsie": 2301,
+      "ethan": 2302,
+      "aiden": 2303
+    },
+    "thinker_hidden_size": 2048,
+    "video_token_id": 151656,
+    "vision_start_token_id": 151652
+  },
+  "thinker_config": {
+    "audio_config": {
+      "_name_or_path": "",
+      "activation_dropout": 0,
+      "activation_function": "gelu",
+      "add_cross_attention": false,
+      "architectures": null,
+      "attention_dropout": 0,
+      "bad_words_ids": null,
+      "begin_suppress_tokens": null,
+      "bos_token_id": null,
+      "chunk_size_feed_forward": 0,
+      "conv_chunksize": 500,
+      "cross_attention_hidden_size": null,
+      "d_model": 1280,
+      "decoder_start_token_id": null,
+      "diversity_penalty": 0.0,
+      "do_sample": false,
+      "downsample_hidden_size":480,
+      "dropout": 0,
+      "dtype": null,
+      "early_stopping": false,
+      "encoder_attention_heads": 20,
+      "encoder_ffn_dim": 5120,
+      "encoder_layers": 32,
+      "encoder_no_repeat_ngram_size": 0,
+      "eos_token_id": null,
+      "exponential_decay_length_penalty": null,
+      "finetuning_task": null,
+      "forced_bos_token_id": null,
+      "forced_eos_token_id": null,
+      "id2label": {
+        "0": "LABEL_0",
+        "1": "LABEL_1"
+      },
+      "initializer_range": 0.02,
+      "is_decoder": false,
+      "is_encoder_decoder": false,
+      "label2id": {
+        "LABEL_0": 0,
+        "LABEL_1": 1
+      },
+      "length_penalty": 1.0,
+      "max_length": 20,
+      "max_source_positions": 1500,
+      "min_length": 0,
+      "model_type": "qwen3_omni_moe_audio_encoder",
+      "n_window": 50,
+      "n_window_infer": 800,
+      "no_repeat_ngram_size": 0,
+      "num_beam_groups": 1,
+      "num_beams": 1,
+      "num_hidden_layers": 32,
+      "num_mel_bins": 128,
+      "num_return_sequences": 1,
+      "output_attentions": false,
+      "output_dim": 2048,
+      "output_hidden_states": false,
+      "output_scores": false,
+      "pad_token_id": null,
+      "prefix": null,
+      "problem_type": null,
+      "pruned_heads": {},
+      "remove_invalid_values": false,
+      "repetition_penalty": 1.0,
+      "return_dict": true,
+      "return_dict_in_generate": false,
+      "scale_embedding": false,
+      "sep_token_id": null,
+      "suppress_tokens": null,
+      "task_specific_params": null,
+      "temperature": 1.0,
+      "tf_legacy_loss": false,
+      "tie_encoder_decoder": false,
+      "tie_word_embeddings": true,
+      "tokenizer_class": null,
+      "top_k": 50,
+      "top_p": 1.0,
+      "torchscript": false,
+      "typical_p": 1.0,
+      "use_bfloat16": false
+    },
+    "audio_end_token_id": 151670,
+    "audio_start_token_id": 151669,
+    "audio_token_id": 151675,
+    "dtype": "bfloat16",
+    "image_token_id": 151655,
+    "initializer_range": 0.02,
+    "model_type": "qwen3_omni_moe_thinker",
+    "position_id_per_seconds": 13,
+    "seconds_per_chunk": 2,
+    "text_config": {
+      "_name_or_path": "",
+      "add_cross_attention": false,
+      "architectures": null,
+      "attention_bias": false,
+      "attention_dropout": 0.0,
+      "bad_words_ids": null,
+      "begin_suppress_tokens": null,
+      "bos_token_id": null,
+      "chunk_size_feed_forward": 0,
+      "cross_attention_hidden_size": null,
+      "decoder_sparse_step": 1,
+      "decoder_start_token_id": null,
+      "diversity_penalty": 0.0,
+      "do_sample": false,
+      "dtype": null,
+      "early_stopping": false,
+      "encoder_no_repeat_ngram_size": 0,
+      "eos_token_id": null,
+      "exponential_decay_length_penalty": null,
+      "finetuning_task": null,
+      "forced_bos_token_id": null,
+      "forced_eos_token_id": null,
+      "head_dim": 128,
+      "hidden_act": "silu",
+      "hidden_size": 2048,
+      "id2label": {
+        "0": "LABEL_0",
+        "1": "LABEL_1"
+      },
+      "initializer_range": 0.02,
+      "intermediate_size": 768,
+      "is_decoder": false,
+      "is_encoder_decoder": false,
+      "label2id": {
+        "LABEL_0": 0,
+        "LABEL_1": 1
+      },
+      "length_penalty": 1.0,
+      "max_length": 20,
+      "max_position_embeddings": 65536,
+      "min_length": 0,
+      "mlp_only_layers": [],
+      "model_type": "qwen3_omni_moe_text",
+      "moe_intermediate_size": 768,
+      "no_repeat_ngram_size": 0,
+      "norm_topk_prob": true,
+      "num_attention_heads": 32,
+      "num_beam_groups": 1,
+      "num_beams": 1,
+      "num_experts": 128,
+      "num_experts_per_tok": 8,
+      "num_hidden_layers": 48,
+      "num_key_value_heads": 4,
+      "num_return_sequences": 1,
+      "output_attentions": false,
+      "output_hidden_states": false,
+      "output_router_logits": false,
+      "output_scores": false,
+      "pad_token_id": null,
+      "prefix": null,
+      "problem_type": null,
+      "pruned_heads": {},
+      "remove_invalid_values": false,
+      "repetition_penalty": 1.0,
+      "return_dict": true,
+      "return_dict_in_generate": false,
+      "rms_norm_eps": 1e-06,
+      "rope_scaling": {
+        "interleaved": true,
+        "mrope_interleaved": true,
+        "mrope_section": [
+          24,
+          20,
+          20
+        ],
+        "rope_type": "default",
+        "type": "default"
+      },
+      "rope_theta": 1000000,
+      "router_aux_loss_coef": 0.001,
+      "sep_token_id": null,
+      "shared_expert_intermediate_size": 0,
+      "sliding_window": null,
+      "suppress_tokens": null,
+      "task_specific_params": null,
+      "temperature": 1.0,
+      "tf_legacy_loss": false,
+      "tie_encoder_decoder": false,
+      "tie_word_embeddings": false,
+      "tokenizer_class": null,
+      "top_k": 50,
+      "top_p": 1.0,
+      "torchscript": false,
+      "typical_p": 1.0,
+      "use_bfloat16": false,
+      "use_cache": true,
+      "use_qk_norm": true,
+      "use_sliding_window": false,
+      "vocab_size": 152064
+    },
+    "user_token_id": 872,
+    "video_token_id": 151656,
+    "vision_config": {
+      "_name_or_path": "",
+      "add_cross_attention": false,
+      "apply_vit_abs_pos_embed": true,
+      "architectures": null,
+      "bad_words_ids": null,
+      "begin_suppress_tokens": null,
+      "bos_token_id": null,
+      "chunk_size_feed_forward": 0,
+      "cross_attention_hidden_size": null,
+      "decoder_start_token_id": null,
+      "deepstack_visual_indexes": [
+        8,
+        16,
+        24
+      ],
+      "depth": 27,
+      "diversity_penalty": 0.0,
+      "do_sample": false,
+      "dtype": null,
+      "early_stopping": false,
+      "encoder_no_repeat_ngram_size": 0,
+      "eos_token_id": null,
+      "exponential_decay_length_penalty": null,
+      "finetuning_task": null,
+      "forced_bos_token_id": null,
+      "forced_eos_token_id": null,
+      "hidden_act": "gelu_pytorch_tanh",
+      "hidden_size": 1152,
+      "id2label": {
+        "0": "LABEL_0",
+        "1": "LABEL_1"
+      },
+      "image_size": 768,
+      "in_channels": 3,
+      "in_chans": 3,
+      "initializer_range": 0.02,
+      "intermediate_size": 4304,
+      "is_decoder": false,
+      "is_encoder_decoder": false,
+      "label2id": {
+        "LABEL_0": 0,
+        "LABEL_1": 1
+      },
+      "length_penalty": 1.0,
+      "max_length": 20,
+      "min_length": 0,
+      "model_type": "qwen3_omni_moe_vision_encoder",
+      "no_repeat_ngram_size": 0,
+      "num_beam_groups": 1,
+      "num_beams": 1,
+      "num_heads": 16,
+      "num_return_sequences": 1,
+      "out_hidden_size": 2048,
+      "output_attentions": false,
+      "output_hidden_states": false,
+      "output_scores": false,
+      "pad_token_id": null,
+      "patch_size": 16,
+      "prefix": null,
+      "problem_type": null,
+      "pruned_heads": {},
+      "remove_invalid_values": false,
+      "repetition_penalty": 1.0,
+      "return_dict": true,
+      "return_dict_in_generate": false,
+      "sep_token_id": null,
+      "spatial_merge_size": 2,
+      "spatial_patch_size": 16,
+      "suppress_tokens": null,
+      "task_specific_params": null,
+      "temperature": 1.0,
+      "temporal_patch_size": 2,
+      "tf_legacy_loss": false,
+      "tie_encoder_decoder": false,
+      "tie_word_embeddings": true,
+      "tokenizer_class": null,
+      "tokens_per_second": 2,
+      "top_k": 50,
+      "top_p": 1.0,
+      "torchscript": false,
+      "typical_p": 1.0,
+      "use_bfloat16": false
+    },
+    "vision_end_token_id": 151653,
+    "vision_start_token_id": 151652
+  },
+  "transformers_version": "4.57.0.dev0",
+  "tts_bos_token_id": 151672,
+  "tts_eos_token_id": 151673,
+  "tts_pad_token_id": 151671,
+  "user_token_id": 872
+}
diff --git a/backend/model_analyzer.py b/backend/model_analyzer.py
index 31e882c..d09a8f7 100644
--- a/backend/model_analyzer.py
+++ b/backend/model_analyzer.py
@@ -248,24 +248,26 @@ class LLMAnalyzer(ModelAnalyzer):
 
         # for attention
         head_size = hidden_size // num_attention_heads
+        tp_num_attention_heads = max(1, num_attention_heads // tp_size)
+        tp_num_key_value_heads = max(1, num_key_value_heads // tp_size)
         # for decode
-        qk_matmul_OPs = seqlen * head_size * num_attention_heads * batchsize * 2
-        sv_matmul_OPs = 1 * head_size * seqlen * num_attention_heads * batchsize * 2
+        qk_matmul_OPs = seqlen * head_size * tp_num_attention_heads * batchsize * 2
+        sv_matmul_OPs = 1 * head_size * seqlen * tp_num_attention_heads * batchsize * 2
         # the softmax operation takes five steps:
         # max_x=max(x)
         # x=x-max_x
         # x_exp=exp(x)
         # sum_x_exp=sum(x_exp)
         # y=x_exp/sum(x_exp)
-        softmax_OPs = batchsize * num_attention_heads * seqlen * 1 * 5
+        softmax_OPs = batchsize * tp_num_attention_heads * seqlen * 1 * 5
         if use_flashattention:
             name = f"fused_attention"
             bandwidth, max_OPS, onchip_buffer = get_hardware_info(self.hardware, self.w_bit, self.a_bit, self.kv_bit)
             # flashattention-2 https://arxiv.org/pdf/2307.08691.pdf
             block_size_r = min(math.ceil(onchip_buffer / (kv_byte * head_size)), head_size)
             n_blocks_r = math.ceil(1 / block_size_r)
-            q_numel = (1) * head_size * batchsize * num_attention_heads * a_byte
-            o_numel = 1 * seqlen * batchsize * num_attention_heads * a_byte
+            q_numel = (1) * head_size * batchsize * tp_num_attention_heads * a_byte
+            o_numel = 1 * seqlen * batchsize * tp_num_attention_heads * a_byte
             self._analyze_to_results(
                 "decode",
                 name,
@@ -273,7 +275,7 @@ class LLMAnalyzer(ModelAnalyzer):
                 load_weight=0,
                 load_act=q_numel,
                 store_act=o_numel * 2,  # initialize O and save O
-                load_kv_cache=n_blocks_r * (seqlen) * head_size * batchsize * num_key_value_heads * kv_byte * 2,
+                load_kv_cache=n_blocks_r * (seqlen) * head_size * batchsize * tp_num_key_value_heads * kv_byte * 2,
                 store_kv_cache=0,
             )
 
@@ -284,9 +286,9 @@ class LLMAnalyzer(ModelAnalyzer):
                 name,
                 OPs=qk_matmul_OPs,
                 load_weight=0,
-                load_act=(1) * head_size * batchsize * num_attention_heads * a_byte,
-                store_act=1 * seqlen * batchsize * num_attention_heads * a_byte,
-                load_kv_cache=(seqlen) * head_size * batchsize * num_key_value_heads * kv_byte,
+                load_act=(1) * head_size * batchsize * tp_num_attention_heads * a_byte,
+                store_act=1 * seqlen * batchsize * tp_num_attention_heads * a_byte,
+                load_kv_cache=(seqlen) * head_size * batchsize * tp_num_key_value_heads * kv_byte,
                 store_kv_cache=0,
             )
             name = f"sv_matmul"
@@ -295,9 +297,9 @@ class LLMAnalyzer(ModelAnalyzer):
                 name,
                 OPs=sv_matmul_OPs,
                 load_weight=0,
-                load_act=(1 * seqlen * batchsize * num_attention_heads) * a_byte,
-                store_act=1 * head_size * batchsize * num_attention_heads * a_byte,
-                load_kv_cache=(seqlen * head_size * batchsize * num_key_value_heads) * kv_byte,
+                load_act=(1 * seqlen * batchsize * tp_num_attention_heads) * a_byte,
+                store_act=1 * head_size * batchsize * tp_num_attention_heads * a_byte,
+                load_kv_cache=(seqlen * head_size * batchsize * tp_num_key_value_heads) * kv_byte,
                 store_kv_cache=0,
             )
 
@@ -308,8 +310,8 @@ class LLMAnalyzer(ModelAnalyzer):
                 name,
                 OPs=softmax_OPs,
                 load_weight=0,
-                load_act=batchsize * num_attention_heads * seqlen * 1 * a_byte,
-                store_act=batchsize * num_attention_heads * seqlen * 1 * a_byte,
+                load_act=batchsize * tp_num_attention_heads * seqlen * 1 * a_byte,
+                store_act=batchsize * tp_num_attention_heads * seqlen * 1 * a_byte,
                 load_kv_cache=0,
                 store_kv_cache=0,
             )
@@ -356,17 +358,17 @@ class LLMAnalyzer(ModelAnalyzer):
             )
 
         # for prefill
-        qk_matmul_OPs = seqlen * seqlen * head_size * num_attention_heads * batchsize * 2
-        sv_matmul_OPs = seqlen * head_size * seqlen * num_attention_heads * batchsize * 2
-        softmax_OPs = batchsize * num_attention_heads * seqlen * seqlen * 5
+        qk_matmul_OPs = seqlen * seqlen * head_size * tp_num_attention_heads * batchsize * 2
+        sv_matmul_OPs = seqlen * head_size * seqlen * tp_num_attention_heads * batchsize * 2
+        softmax_OPs = batchsize * tp_num_attention_heads * seqlen * seqlen * 5
         if use_flashattention:
             name = f"fused_attention"
             bandwidth, max_OPS, onchip_buffer = get_hardware_info(self.hardware, self.w_bit, self.a_bit, self.kv_bit)
             # flashattention-2 https://arxiv.org/pdf/2307.08691.pdf
             block_size_r = min(math.ceil(onchip_buffer / (kv_byte * head_size)), head_size)
             n_blocks_r = math.ceil(seqlen / block_size_r)
-            q_numel = seqlen * head_size * batchsize * num_attention_heads * a_byte
-            o_numel = seqlen * seqlen * batchsize * num_attention_heads * a_byte
+            q_numel = seqlen * head_size * batchsize * tp_num_attention_heads * a_byte
+            o_numel = seqlen * seqlen * batchsize * tp_num_attention_heads * a_byte
             self._analyze_to_results(
                 "prefill",
                 name,
@@ -374,7 +376,7 @@ class LLMAnalyzer(ModelAnalyzer):
                 load_weight=0,
                 load_act=q_numel,
                 store_act=o_numel * 2,  # initialize O and save O
-                load_kv_cache=n_blocks_r * (seqlen) * head_size * batchsize * num_key_value_heads * kv_byte * 2,
+                load_kv_cache=n_blocks_r * (seqlen) * head_size * batchsize * tp_num_key_value_heads * kv_byte * 2,
                 store_kv_cache=0,
             )
         else:
@@ -384,9 +386,9 @@ class LLMAnalyzer(ModelAnalyzer):
                 name,
                 OPs=qk_matmul_OPs,
                 load_weight=0,
-                load_act=seqlen * head_size * batchsize * num_key_value_heads * a_byte,
-                store_act=seqlen * seqlen * batchsize * num_attention_heads * a_byte,
-                load_kv_cache=seqlen * head_size * batchsize * num_key_value_heads * kv_byte,
+                load_act=seqlen * head_size * batchsize * tp_num_key_value_heads * a_byte,
+                store_act=seqlen * seqlen * batchsize * tp_num_attention_heads * a_byte,
+                load_kv_cache=seqlen * head_size * batchsize * tp_num_key_value_heads * kv_byte,
                 store_kv_cache=0,
             )
             name = f"sv_matmul"
@@ -395,9 +397,9 @@ class LLMAnalyzer(ModelAnalyzer):
                 name,
                 OPs=sv_matmul_OPs,
                 load_weight=0,
-                load_act=seqlen * seqlen * batchsize * num_attention_heads * a_byte,
-                store_act=seqlen * head_size * batchsize * num_attention_heads * a_byte,
-                load_kv_cache=seqlen * head_size * batchsize * num_key_value_heads * kv_byte,
+                load_act=seqlen * seqlen * batchsize * tp_num_attention_heads * a_byte,
+                store_act=seqlen * head_size * batchsize * tp_num_attention_heads * a_byte,
+                load_kv_cache=seqlen * head_size * batchsize * tp_num_key_value_heads * kv_byte,
                 store_kv_cache=0,
             )
             name = f"softmax"
@@ -406,8 +408,8 @@ class LLMAnalyzer(ModelAnalyzer):
                 name,
                 OPs=softmax_OPs,
                 load_weight=0,
-                load_act=batchsize * num_attention_heads * seqlen * seqlen * a_byte,
-                store_act=batchsize * num_attention_heads * seqlen * seqlen * a_byte,
+                load_act=batchsize * tp_num_attention_heads * seqlen * seqlen * a_byte,
+                store_act=batchsize * tp_num_attention_heads * seqlen * seqlen * a_byte,
                 load_kv_cache=0,
                 store_kv_cache=0,
             )
@@ -570,24 +572,26 @@ class MoEAnalyzer(ModelAnalyzer):
 
         # for attention
         head_size = hidden_size // num_attention_heads
+        tp_num_attention_heads = max(1, num_attention_heads // tp_size)
+        tp_num_key_value_heads = max(1, num_key_value_heads // tp_size)
         # for decode
-        qk_matmul_OPs = seqlen * head_size * num_attention_heads * batchsize * 2
-        sv_matmul_OPs = 1 * head_size * seqlen * num_attention_heads * batchsize * 2
+        qk_matmul_OPs = seqlen * head_size * tp_num_attention_heads * batchsize * 2
+        sv_matmul_OPs = 1 * head_size * seqlen * tp_num_attention_heads * batchsize * 2
         # the softmax operation takes five steps:
         # max_x=max(x)
         # x=x-max_x
         # x_exp=exp(x)
         # sum_x_exp=sum(x_exp)
         # y=x_exp/sum(x_exp)
-        softmax_OPs = batchsize * num_attention_heads * seqlen * 1 * 5
+        softmax_OPs = batchsize * tp_num_attention_heads * seqlen * 1 * 5
         if use_flashattention:
             name = f"fused_attention"
             bandwidth, max_OPS, onchip_buffer = get_hardware_info(self.hardware, self.w_bit, self.a_bit, self.kv_bit)
             # flashattention-2 https://arxiv.org/pdf/2307.08691.pdf
             block_size_r = min(math.ceil(onchip_buffer / (kv_byte * head_size)), head_size)
             n_blocks_r = math.ceil(1 / block_size_r)
-            q_numel = (1) * head_size * batchsize * num_attention_heads * a_byte
-            o_numel = 1 * seqlen * batchsize * num_attention_heads * a_byte
+            q_numel = (1) * head_size * batchsize * tp_num_attention_heads * a_byte
+            o_numel = 1 * seqlen * batchsize * tp_num_attention_heads * a_byte
             self._analyze_to_results(
                 "decode",
                 name,
@@ -595,7 +599,7 @@ class MoEAnalyzer(ModelAnalyzer):
                 load_weight=0,
                 load_act=q_numel,
                 store_act=o_numel * 2,  # initialize O and save O
-                load_kv_cache=n_blocks_r * (seqlen) * head_size * batchsize * num_key_value_heads * kv_byte * 2,
+                load_kv_cache=n_blocks_r * (seqlen) * head_size * batchsize * tp_num_key_value_heads * kv_byte * 2,
                 store_kv_cache=0,
             )
 
@@ -606,9 +610,9 @@ class MoEAnalyzer(ModelAnalyzer):
                 name,
                 OPs=qk_matmul_OPs,
                 load_weight=0,
-                load_act=(1) * head_size * batchsize * num_attention_heads * a_byte,
-                store_act=1 * seqlen * batchsize * num_attention_heads * a_byte,
-                load_kv_cache=(seqlen) * head_size * batchsize * num_key_value_heads * kv_byte,
+                load_act=(1) * head_size * batchsize * tp_num_attention_heads * a_byte,
+                store_act=1 * seqlen * batchsize * tp_num_attention_heads * a_byte,
+                load_kv_cache=(seqlen) * head_size * batchsize * tp_num_key_value_heads * kv_byte,
                 store_kv_cache=0,
             )
             name = f"sv_matmul"
@@ -617,9 +621,9 @@ class MoEAnalyzer(ModelAnalyzer):
                 name,
                 OPs=sv_matmul_OPs,
                 load_weight=0,
-                load_act=(1 * seqlen * batchsize * num_attention_heads) * a_byte,
-                store_act=1 * head_size * batchsize * num_attention_heads * a_byte,
-                load_kv_cache=(seqlen * head_size * batchsize * num_key_value_heads) * kv_byte,
+                load_act=(1 * seqlen * batchsize * tp_num_attention_heads) * a_byte,
+                store_act=1 * head_size * batchsize * tp_num_attention_heads * a_byte,
+                load_kv_cache=(seqlen * head_size * batchsize * tp_num_key_value_heads) * kv_byte,
                 store_kv_cache=0,
             )
 
@@ -630,8 +634,8 @@ class MoEAnalyzer(ModelAnalyzer):
                 name,
                 OPs=softmax_OPs,
                 load_weight=0,
-                load_act=batchsize * num_attention_heads * seqlen * 1 * a_byte,
-                store_act=batchsize * num_attention_heads * seqlen * 1 * a_byte,
+                load_act=batchsize * tp_num_attention_heads * seqlen * 1 * a_byte,
+                store_act=batchsize * tp_num_attention_heads * seqlen * 1 * a_byte,
                 load_kv_cache=0,
                 store_kv_cache=0,
             )
@@ -678,17 +682,17 @@ class MoEAnalyzer(ModelAnalyzer):
             )
 
         # for prefill
-        qk_matmul_OPs = seqlen * seqlen * head_size * num_attention_heads * batchsize * 2
-        sv_matmul_OPs = seqlen * head_size * seqlen * num_attention_heads * batchsize * 2
-        softmax_OPs = batchsize * num_attention_heads * seqlen * seqlen * 5
+        qk_matmul_OPs = seqlen * seqlen * head_size * tp_num_attention_heads * batchsize * 2
+        sv_matmul_OPs = seqlen * head_size * seqlen * tp_num_attention_heads * batchsize * 2
+        softmax_OPs = batchsize * tp_num_attention_heads * seqlen * seqlen * 5
         if use_flashattention:
             name = f"fused_attention"
             bandwidth, max_OPS, onchip_buffer = get_hardware_info(self.hardware, self.w_bit, self.a_bit, self.kv_bit)
             # flashattention-2 https://arxiv.org/pdf/2307.08691.pdf
             block_size_r = min(math.ceil(onchip_buffer / (kv_byte * head_size)), head_size)
             n_blocks_r = math.ceil(seqlen / block_size_r)
-            q_numel = seqlen * head_size * batchsize * num_attention_heads * a_byte
-            o_numel = seqlen * seqlen * batchsize * num_attention_heads * a_byte
+            q_numel = seqlen * head_size * batchsize * tp_num_attention_heads * a_byte
+            o_numel = seqlen * seqlen * batchsize * tp_num_attention_heads * a_byte
             self._analyze_to_results(
                 "prefill",
                 name,
@@ -696,7 +700,7 @@ class MoEAnalyzer(ModelAnalyzer):
                 load_weight=0,
                 load_act=q_numel,
                 store_act=o_numel * 2,  # initialize O and save O
-                load_kv_cache=n_blocks_r * (seqlen) * head_size * batchsize * num_key_value_heads * kv_byte * 2,
+                load_kv_cache=n_blocks_r * (seqlen) * head_size * batchsize * tp_num_key_value_heads * kv_byte * 2,
                 store_kv_cache=0,
             )
         else:
@@ -706,9 +710,9 @@ class MoEAnalyzer(ModelAnalyzer):
                 name,
                 OPs=qk_matmul_OPs,
                 load_weight=0,
-                load_act=seqlen * head_size * batchsize * num_key_value_heads * a_byte,
-                store_act=seqlen * seqlen * batchsize * num_attention_heads * a_byte,
-                load_kv_cache=seqlen * head_size * batchsize * num_key_value_heads * kv_byte,
+                load_act=seqlen * head_size * batchsize * tp_num_key_value_heads * a_byte,
+                store_act=seqlen * seqlen * batchsize * tp_num_attention_heads * a_byte,
+                load_kv_cache=seqlen * head_size * batchsize * tp_num_key_value_heads * kv_byte,
                 store_kv_cache=0,
             )
             name = f"sv_matmul"
@@ -717,9 +721,9 @@ class MoEAnalyzer(ModelAnalyzer):
                 name,
                 OPs=sv_matmul_OPs,
                 load_weight=0,
-                load_act=seqlen * seqlen * batchsize * num_attention_heads * a_byte,
-                store_act=seqlen * head_size * batchsize * num_attention_heads * a_byte,
-                load_kv_cache=seqlen * head_size * batchsize * num_key_value_heads * kv_byte,
+                load_act=seqlen * seqlen * batchsize * tp_num_attention_heads * a_byte,
+                store_act=seqlen * head_size * batchsize * tp_num_attention_heads * a_byte,
+                load_kv_cache=seqlen * head_size * batchsize * tp_num_key_value_heads * kv_byte,
                 store_kv_cache=0,
             )
             name = f"softmax"
@@ -728,8 +732,8 @@ class MoEAnalyzer(ModelAnalyzer):
                 name,
                 OPs=softmax_OPs,
                 load_weight=0,
-                load_act=batchsize * num_attention_heads * seqlen * seqlen * a_byte,
-                store_act=batchsize * num_attention_heads * seqlen * seqlen * a_byte,
+                load_act=batchsize * tp_num_attention_heads * seqlen * seqlen * a_byte,
+                store_act=batchsize * tp_num_attention_heads * seqlen * seqlen * a_byte,
                 load_kv_cache=0,
                 store_kv_cache=0,
             )
-- 
2.34.1

